{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "9cc5969f6ecb2ccf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:23:18.041791Z",
     "start_time": "2025-05-08T15:23:18.037946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from time import time\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gymnasium.envs.toy_text.blackjack import BlackjackEnv\n",
    "\n",
    "from agent import QLearningAgent\n",
    "from src.q_learning.blackjack_util import get_policy_from_qtable, get_mdp_policy_from_gym_policy, get_values_from_qtable, map_to_state_indexes\n",
    "from src.q_learning.blackjack_util import play_using_policy\n",
    "from src.q_learning.visualize_policy import create_plots, create_grids\n",
    "from q_learning import train_qlearning_agent as train_QLearning_agent"
   ],
   "id": "9128162e0715cd4f",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Functions",
   "id": "a5dfa37caa0c5f99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:23:20.311480Z",
     "start_time": "2025-05-08T15:23:20.307231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_agent_on_blackjack(FL_ENV, learning_rate=0.01, n_episodes=100000, start_epsilon=1.0, epsilon_decay=0.001, final_epsilon=0.1, gamma=0.95, decay='linear'):\n",
    "    agent = QLearningAgent(\n",
    "        FL_ENV.action_space.n,\n",
    "        learning_rate=learning_rate,\n",
    "        initial_epsilon=start_epsilon,\n",
    "        epsilon_decay=epsilon_decay,\n",
    "        final_epsilon=final_epsilon,\n",
    "        discount_factor=gamma,\n",
    "        decay=decay,\n",
    "        init_q_values='zero'\n",
    "    )\n",
    "\n",
    "    env_wrapper = train_QLearning_agent(agent, FL_ENV, n_episodes=n_episodes)\n",
    "\n",
    "    return agent, env_wrapper\n",
    "\n",
    "\n",
    "def get_policy_from_q_learning(q_table, states):\n",
    "    return [np.argmax(q_table[s]) for s in states]"
   ],
   "id": "c6e852ad09ea2a28",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Parameters",
   "id": "88087ff36df25a62"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:54:15.814349Z",
     "start_time": "2025-05-08T15:54:15.810006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "LR = [0.01, 0.1, 0.15]\n",
    "GAMMAS = [0.8, 0.95, 0.99]\n",
    "N_EPISODES = [120_000, 150_000]\n",
    "EPSILON = [(0.9, 0.1, 0.002), (0.8, 0.2, 0.01)]"
   ],
   "id": "ea93f22b17a57e2a",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Final",
   "id": "66709188176eb594"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "blackjack_env = BlackjackEnv()\n",
    "runs_dict = {}\n",
    "\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "for learning_rate in LR:\n",
    "    for n_episodes in N_EPISODES:\n",
    "        for gamma in GAMMAS:\n",
    "            for start_epsilon, final_epsilon, epsilon_step in EPSILON:\n",
    "                start_time = time()\n",
    "                agent, env_wrapper = train_agent_on_blackjack(blackjack_env, learning_rate=learning_rate, n_episodes=n_episodes, start_epsilon=start_epsilon, epsilon_decay=epsilon_step, final_epsilon=final_epsilon, gamma=0.95, decay='linear')\n",
    "                exec_time = time() - start_time\n",
    "                policy = get_policy_from_qtable(agent.q_values)\n",
    "                values = get_values_from_qtable(agent.q_values)\n",
    "                values_mdp = map_to_state_indexes(values)\n",
    "                mdp_policy = get_mdp_policy_from_gym_policy(policy)\n",
    "                print(f'\\n******************* Game plays for lr_{learning_rate}-gamma_{gamma} *******************')\n",
    "                policy_grid, values_grid = create_grids(agent, usable_ace=True)\n",
    "                fig = create_plots(values_grid, policy_grid, rf'Q-Learning Policy for Blackjack with $\\gamma$={gamma}, $\\alpha$={learning_rate}, start $\\epsilon$={start_epsilon}, final $\\epsilon$={final_epsilon}, $\\epsilon$ step={epsilon_step} and {n_episodes} episodes')\n",
    "                fig.savefig(f\"plots/learning_rate_{learning_rate}__gamma_{gamma}__n_episodes_{n_episodes}__epsilon_start_step_final_{start_epsilon}_{final_epsilon}_{epsilon_step}.png\", bbox_inches='tight')\n",
    "                # plt.show()\n",
    "                play_using_policy(BlackjackEnv(), policy, games=1000)"
   ],
   "id": "18f5f4d5db108d42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c1d177ee88a230ba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
